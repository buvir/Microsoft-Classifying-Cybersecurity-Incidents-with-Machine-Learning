{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fda740",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Microsoft Cybersecurity Incident Classification\n",
    "# ## Classifying incidents as TP, BP, or FP using Machine Learning\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Environment Setup and Data Download\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %%\n",
    "# Create data directory if it doesn't exist\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "# %%\n",
    "def download_and_extract_zip(url, filename):\n",
    "    \"\"\"Download and extract zip file from Google Drive\"\"\"\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, stream=True)\n",
    "    \n",
    "    # Handle large file download\n",
    "    zip_path = f\"data/{filename}\"\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    print(f\"Extracting {filename}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "    \n",
    "    print(f\"Completed {filename} processing\")\n",
    "    return f\"data/{filename.replace('.zip', '.csv')}\"\n",
    "\n",
    "# %%\n",
    "# Direct download links (replace with your actual links)\n",
    "test_zip_url = 'https://drive.google.com/uc?export=download&id=1jtBLurdvjkBpzBhABJsRspNuN-mPs6Z5'\n",
    "train_zip_url = 'https://drive.google.com/uc?export=download&id=1wKJgSOGjjKQh2CO3WaEi5L0S0ZuMnbUR'\n",
    "\n",
    "# Download and extract files\n",
    "test_csv_path = download_and_extract_zip(test_zip_url, 'test.zip')\n",
    "train_csv_path = download_and_extract_zip(train_zip_url, 'train.zip')\n",
    "\n",
    "# %%\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "# %%\n",
    "# Initial inspection\n",
    "print(\"Train dataset info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "# %%\n",
    "# Target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='triage_grade', data=train_df)\n",
    "plt.title('Distribution of Triage Grades in Training Data')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "print(\"Missing values:\\n\", missing_values)\n",
    "\n",
    "# %%\n",
    "# Numerical features analysis\n",
    "numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "\n",
    "# Plot distributions for some numerical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols[:6]):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(train_df[col], bins=30, kde=True)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Categorical features analysis\n",
    "categorical_cols = train_df.select_dtypes(include=['object']).columns.drop('triage_grade')\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Plot value counts for some categorical features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(categorical_cols[:6]):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.countplot(x=col, data=train_df, order=train_df[col].value_counts().iloc[:10].index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Preprocessing\n",
    "\n",
    "# %%\n",
    "# Define features and target\n",
    "X_train = train_df.drop(columns=['triage_grade'])\n",
    "y_train = train_df['triage_grade']\n",
    "X_test = test_df.drop(columns=['triage_grade'])\n",
    "y_test = test_df['triage_grade']\n",
    "\n",
    "# %%\n",
    "# Identify feature types\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# %%\n",
    "# Preprocessing pipeline\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Model Training\n",
    "\n",
    "# %%\n",
    "# Class weights calculation (to handle imbalance)\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# %%\n",
    "# Baseline Random Forest model\n",
    "rf_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight=class_weight_dict,\n",
    "        random_state=42,\n",
    "        n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# XGBoost model\n",
    "xgb_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        objective='multi:softmax',\n",
    "        random_state=42,\n",
    "        n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Evaluate model and print metrics\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name} model...\")\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred, target_names=classes))\n",
    "    \n",
    "    macro_f1 = f1_score(y, y_pred, average='macro')\n",
    "    precision = precision_score(y, y_pred, average='macro')\n",
    "    recall = recall_score(y, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"\\nMacro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Macro Precision: {precision:.4f}\")\n",
    "    print(f\"Macro Recall: {recall:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'macro_f1': macro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# Evaluate on validation set (20% of training data)\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "rf_metrics = evaluate_model(rf_model, X_val, y_val, \"Random Forest\")\n",
    "xgb_metrics = evaluate_model(xgb_model, X_val, y_val, \"XGBoost\")\n",
    "\n",
    "# %%\n",
    "# Final evaluation on test set\n",
    "print(\"\\n\\n=== Final Test Evaluation ===\")\n",
    "rf_test_metrics = evaluate_model(rf_model, X_test, y_test, \"Random Forest (Test)\")\n",
    "xgb_test_metrics = evaluate_model(xgb_model, X_test, y_test, \"XGBoost (Test)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Model Interpretation\n",
    "\n",
    "# %%\n",
    "# Feature importance for Random Forest\n",
    "if hasattr(rf_model.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = (numerical_features.tolist() + \n",
    "                    list(rf_model.named_steps['preprocessor']\n",
    "                        .named_transformers_['cat']\n",
    "                        .get_feature_names_out(categorical_features)))\n",
    "    \n",
    "    importances = rf_model.named_steps['classifier'].feature_importances_\n",
    "    indices = np.argsort(importances)[-20:]  # Top 20 features\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title('Random Forest - Top 20 Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Model Saving\n",
    "\n",
    "# %%\n",
    "# Save the best performing model\n",
    "print(\"\\nSaving the best model...\")\n",
    "joblib.dump(xgb_model, 'cybersecurity_incident_classifier.pkl')\n",
    "\n",
    "# %%\n",
    "# Save evaluation metrics\n",
    "metrics_df = pd.DataFrame([rf_metrics, xgb_metrics, rf_test_metrics, xgb_test_metrics])\n",
    "metrics_df.to_csv('model_evaluation_metrics.csv', index=False)\n",
    "print(\"Evaluation metrics saved to model_evaluation_metrics.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Deployment Recommendations\n",
    "\n",
    "# %%\n",
    "print(\"\"\"\n",
    "## Deployment Recommendations:\n",
    "\n",
    "1. **Model Integration**: \n",
    "   - Integrate the trained model into SOC workflows as a triage assistant\n",
    "   - Use model predictions to prioritize incident review queues\n",
    "\n",
    "2. **Monitoring**:\n",
    "   - Implement continuous performance monitoring\n",
    "   - Set up alerts for model drift or performance degradation\n",
    "\n",
    "3. **Feedback Loop**:\n",
    "   - Collect analyst feedback on predictions\n",
    "   - Use this to periodically retrain the model\n",
    "\n",
    "4. **Scalability**:\n",
    "   - For production, consider containerizing the model\n",
    "   - Use a microservices architecture for scalability\n",
    "\n",
    "5. **Security**:\n",
    "   - Ensure model API has proper authentication\n",
    "   - Log all prediction requests for audit purposes\n",
    "\"\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Project Documentation\n",
    "\n",
    "print(\"\"\"\n",
    "# Project Documentation: Microsoft Cybersecurity Incident Classification\n",
    "\n",
    "## Overview\n",
    "This project developed a machine learning model to classify cybersecurity incidents as:\n",
    "- True Positive (TP)\n",
    "- Benign Positive (BP)\n",
    "- False Positive (FP)\n",
    "\n",
    "## Key Steps\n",
    "1. Data acquisition and preprocessing\n",
    "2. Exploratory data analysis\n",
    "3. Feature engineering\n",
    "4. Model training (Random Forest and XGBoost)\n",
    "5. Model evaluation\n",
    "6. Model interpretation\n",
    "\n",
    "## Results\n",
    "The best performing model achieved:\n",
    "- Macro F1 Score: {xgb_test_metrics['macro_f1']:.4f}\n",
    "- Precision: {xgb_test_metrics['precision']:.4f}\n",
    "- Recall: {xgb_test_metrics['recall']:.4f}\n",
    "\n",
    "## Files Included\n",
    "1. `cybersecurity_incident_classifier.pkl` - Trained model\n",
    "2. `model_evaluation_metrics.csv` - Performance metrics\n",
    "3. This notebook - Complete project documentation\n",
    "\n",
    "## Future Improvements\n",
    "1. Experiment with deep learning models\n",
    "2. Incorporate more feature engineering\n",
    "3. Implement automated retraining pipeline\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
